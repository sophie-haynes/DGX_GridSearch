{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f090e7-eb2b-4a70-ba01-aafe8301fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8677ef97-870c-4903-9eb9-9d0597ad5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsrtdir = \"/home/local/data/sophie/node21_combined/jsrt/test/\"\n",
    "openidir = \"/home/local/data/sophie/node21_combined/openi/test/\"\n",
    "padtraindir = \"/home/local/data/sophie/node21_combined/padchest/train/\"\n",
    "cxr14traindir = \"/home/local/data/sophie/node21_combined/cxr14/train/\"\n",
    "\n",
    "padtestdir = \"/home/local/data/sophie/node21_combined/padchest/test/\"\n",
    "cxr14testdir = \"/home/local/data/sophie/node21_combined/cxr14/test/\"\n",
    "\n",
    "imgnet_train =\"/home/local/data/sophie/imagenet/imagenet_train/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62832054-804e-4c0f-b1e1-0df59d3cb0e3",
   "metadata": {},
   "source": [
    "## ImageNet Backbone Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a0526d-d707-435a-8a9c-367efb3ea0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interpolation = \"bilinear\"\n",
    "train_crop_size = 224\n",
    "auto_augment_policy = None\n",
    "random_erase_prob = 0.0 \n",
    "augmix_severity = 3\n",
    "ra_magnitude = 9\n",
    "crop_size = train_crop_size\n",
    "base_mean=(0.485, 0.456, 0.406)\n",
    "base_std=(0.229, 0.224, 0.225)\n",
    "\n",
    "grey_mean=(0.449,0.449,0.449)\n",
    "grey_std=(0.236,0.236,0.236)\n",
    "interpolation=InterpolationMode.BILINEAR\n",
    "hflip_prob=0.5\n",
    "auto_augment_policy=None\n",
    "ra_magnitude=9\n",
    "augmix_severity=3\n",
    "random_erase_prob=0.0\n",
    "backend=\"pil\"\n",
    "use_v2=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a7b1f8-9b3e-4d8f-9130-7422ca7c72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = []\n",
    "transforms.append(T.RandomResizedCrop(crop_size, interpolation=interpolation, antialias=True))\n",
    "transforms.append(T.RandomHorizontalFlip(hflip_prob))\n",
    "transforms.append(T.PILToTensor())\n",
    "base_transforms = transforms.copy()\n",
    "grey_transforms = transforms.copy()\n",
    "base_transforms.extend(\n",
    "            [\n",
    "                T.ToDtype(torch.float, scale=True) if use_v2 else T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=base_mean, std=base_std),\n",
    "            ]\n",
    "        )\n",
    "grey_transforms.extend(\n",
    "            [\n",
    "                T.ToDtype(torch.float, scale=True) if use_v2 else T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean=grey_mean, std=grey_std),\n",
    "            ]\n",
    "        )\n",
    "base_comp_transforms = T.Compose(base_transforms)\n",
    "grey_comp_transforms = T.Compose(grey_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d4c989-bfc5-4f1b-99ac-a10adf449784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    PILToTensor()\n",
       "    ConvertImageDtype()\n",
       "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_comp_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5631d5d7-3a52-44e1-8097-e08835a65c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    PILToTensor()\n",
       "    ConvertImageDtype()\n",
       "    Normalize(mean=(0.449, 0.449, 0.449), std=(0.236, 0.236, 0.236))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grey_comp_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c4b527-6168-44b2-a522-819273b3da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97714308-eed2-40bf-8632-626b6d6d9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imgnet_train = torchvision.datasets.ImageFolder(\n",
    "    imgnet_train, transform=base_comp_transforms\n",
    ")\n",
    "grey_imgnet_train = torchvision.datasets.ImageFolder(\n",
    "    imgnet_train, transform=grey_comp_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cc0e00-c8da-41c2-a4c8-6ce65f63e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imgnet_loader = torch.utils.data.DataLoader(base_imgnet_train, batch_size=100, \n",
    "                          shuffle=False)\n",
    "base_imgnet_loader = torch.utils.data.DataLoader(grey_imgnet_train, batch_size=100, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8a685ef-3768-4d92-a0ae-0545690ac49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Base train imgnet\")\n",
    "# tmp_min = 1000000000\n",
    "# tmp_max = -1000000000\n",
    "# for data in base_imgnet_loader:\n",
    "#     inputs, labels = data[0],data[1]\n",
    "#     tmp_min = inputs[0].min() if inputs[0].min()<tmp_min else tmp_min\n",
    "#     tmp_max = inputs[0].max() if inputs[0].max()>tmp_max else tmp_max\n",
    "    \n",
    "# print(f\"Min: {tmp_min} \\\n",
    "# \\nMax: {tmp_max}\")\n",
    "\n",
    "# print(\"Grey train imgnet\")\n",
    "# tmp_min = 1000000000\n",
    "# tmp_max = -1000000000\n",
    "# for data in grey_imgnet_loader:\n",
    "#     inputs, labels = data[0],data[1]\n",
    "#     tmp_min = inputs[0].min() if inputs[0].min()<tmp_min else tmp_min\n",
    "#     tmp_max = inputs[0].max() if inputs[0].max()>tmp_max else tmp_max\n",
    "    \n",
    "# print(f\"Min: {tmp_min} \\\n",
    "# \\nMax: {tmp_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e66f3e9-f508-49bc-8de6-eb121b8607c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3347)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6479d3df-dc2d-4e56-9182-9f8e3ec6f4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.9025)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b2d44-bec9-4cb3-b8a7-ed843ec8ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e53cb3c7-ec8d-4c83-a944-03c80e0a505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14traindir, transform=base_comp_transforms\n",
    ")\n",
    "grey_train_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14traindir, transform=grey_comp_transforms\n",
    ")\n",
    "base_test_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14testdir, transform=base_comp_transforms\n",
    ")\n",
    "grey_test_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14testdir, transform=grey_comp_transforms\n",
    ")\n",
    "\n",
    "base_train_pad = torchvision.datasets.ImageFolder(\n",
    "    padtraindir, transform=base_comp_transforms\n",
    ")\n",
    "grey_train_pad = torchvision.datasets.ImageFolder(\n",
    "    padtraindir, transform=grey_comp_transforms\n",
    ")\n",
    "base_test_pad = torchvision.datasets.ImageFolder(\n",
    "    padtestdir, transform=base_comp_transforms\n",
    ")\n",
    "grey_test_pad = torchvision.datasets.ImageFolder(\n",
    "    padtestdir, transform=grey_comp_transforms\n",
    ")\n",
    "\n",
    "base_openi = torchvision.datasets.ImageFolder(\n",
    "    openidir, transform=base_comp_transforms\n",
    ")\n",
    "grey_openi = torchvision.datasets.ImageFolder(\n",
    "    openidir, transform=grey_comp_transforms\n",
    ")\n",
    "\n",
    "base_jsrt = torchvision.datasets.ImageFolder(\n",
    "    jsrtdir, transform=base_comp_transforms\n",
    ")\n",
    "grey_jsrt = torchvision.datasets.ImageFolder(\n",
    "    jsrtdir, transform=grey_comp_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "89c159f5-b327-43de-9dac-3ef0407e2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cxr14_train_loader = torch.utils.data.DataLoader(base_train_cxr14, batch_size=1638, \n",
    "                          shuffle=False)\n",
    "grey_cxr14_train_loader = torch.utils.data.DataLoader(grey_train_cxr14, batch_size=1638, \n",
    "                          shuffle=False)\n",
    "base_cxr14_test_loader = torch.utils.data.DataLoader(base_test_cxr14, batch_size=360, \n",
    "                          shuffle=False)\n",
    "grey_cxr14_test_loader = torch.utils.data.DataLoader(grey_test_cxr14, batch_size=360, \n",
    "                          shuffle=False)\n",
    "base_pad_loader = torch.utils.data.DataLoader(base_test_pad, batch_size=188, \n",
    "                          shuffle=False)\n",
    "grey_pad_loader = torch.utils.data.DataLoader(grey_test_pad, batch_size=188, \n",
    "                          shuffle=False)\n",
    "base_openi_loader = torch.utils.data.DataLoader(base_openi, batch_size=108, \n",
    "                          shuffle=False)\n",
    "grey_openi_loader = torch.utils.data.DataLoader(grey_openi, batch_size=108, \n",
    "                          shuffle=False)\n",
    "base_jsrt_loader = torch.utils.data.DataLoader(base_jsrt, batch_size=186, \n",
    "                          shuffle=False)\n",
    "grey_jsrt_loader = torch.utils.data.DataLoader(grey_jsrt, batch_size=186, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10b7be3c-a4c7-472d-b2cc-9da2b2dba7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base train cxr14\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.3437039852142334 \n",
      "Mean: 0.6872250437736511\n",
      "Grey train cxr14\n",
      "Min: -1.9025423526763916 \n",
      "Max: 2.1021103858947754 \n",
      "Mean: 0.6690179109573364\n"
     ]
    }
   ],
   "source": [
    "print(\"Base train cxr14\")\n",
    "for data in base_cxr14_train_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")\n",
    "print(\"Grey train cxr14\")\n",
    "for data in grey_cxr14_train_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d50dfb5b-a574-48ed-b02c-f9e8808bf15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base test cxr14\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.273987293243408 \n",
      "Mean: 0.6121335029602051\n",
      "Grey test cxr14\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.2565577030181885 \n",
      "Mean: 0.7057452201843262\n"
     ]
    }
   ],
   "source": [
    "print(\"Base test cxr14\")\n",
    "for data in base_cxr_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")\n",
    "print(\"Grey test cxr14\")\n",
    "for data in grey_cxr_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "403eaac9-60e3-4eb2-988e-430e28299afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base test padchest\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.2914161682128906 \n",
      "Mean: 0.22128352522850037\n",
      "Grey test padchest\n",
      "Min: -1.6720582246780396 \n",
      "Max: 1.638906717300415 \n",
      "Mean: 0.6809470057487488\n"
     ]
    }
   ],
   "source": [
    "print(\"Base test padchest\")\n",
    "for data in base_pad_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")\n",
    "print(\"Grey test padchest\")\n",
    "for data in grey_pad_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "31b92a65-eb53-402d-8650-8490a03e106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base OpenI\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.2565577030181885 \n",
      "Mean: 0.37761855125427246\n",
      "Grey OpenI\n",
      "Min: -1.6720582246780396 \n",
      "Max: 1.638906717300415 \n",
      "Mean: 0.6436389088630676\n"
     ]
    }
   ],
   "source": [
    "print(\"Base OpenI\")\n",
    "for data in base_openi_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")\n",
    "print(\"Grey OpenI\")\n",
    "for data in grey_openi_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f117369d-fd21-40ff-ab0f-bcb93cda900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base JSRT\n",
      "Min: -2.1179039478302 \n",
      "Max: 2.3088455200195312 \n",
      "Mean: 0.6476307511329651\n",
      "Grey JSRT\n",
      "Min: -1.6720582246780396 \n",
      "Max: 1.638906717300415 \n",
      "Mean: 0.7279282212257385\n"
     ]
    }
   ],
   "source": [
    "print(\"Base JSRT\")\n",
    "for data in base_jsrt_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")\n",
    "print(\"Grey JSRT\")\n",
    "for data in grey_jsrt_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6920d7-05e4-4112-90b8-830c5c448cb0",
   "metadata": {},
   "source": [
    "## CXR Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe43e4-6c2b-455c-a163-9afc835ae19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "crop_size = 224\n",
    "mean = (128.2716, 128.2716, 128.2716)\n",
    "std = (76.7148, 76.7148, 76.7148)\n",
    "cxr_transform_list = [\n",
    "    v2.ToImage(),\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomApply([v2.ColorJitter(0.4, 0.2, 0.2,0)], p=0.8),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.RandomResizedCrop(size=crop_size, scale=(0.6, 1.),antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=False),\n",
    "    v2.Normalize(mean=mean, std=std)   \n",
    "]\n",
    "grey_comp_transforms = v2.Compose(cxr_transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1c6a9-a880-43dc-a6a4-2b8cdf36882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_train_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14traindir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_test_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14testdir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_train_pad = torchvision.datasets.ImageFolder(\n",
    "    padtraindir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_test_pad = torchvision.datasets.ImageFolder(\n",
    "    padtestdir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_openi = torchvision.datasets.ImageFolder(\n",
    "    openidir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_jsrt = torchvision.datasets.ImageFolder(\n",
    "    jsrtdir, transform=grey_comp_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0ac96ee-5388-4138-af9d-c9ad93af50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_cxr14_train_loader = torch.utils.data.DataLoader(grey_train_cxr14, batch_size=1638, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_cxr14_test_loader = torch.utils.data.DataLoader(grey_test_cxr14, batch_size=360, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_pad_loader = torch.utils.data.DataLoader(grey_test_pad, batch_size=188, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_openi_loader = torch.utils.data.DataLoader(grey_openi, batch_size=108, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_jsrt_loader = torch.utils.data.DataLoader(grey_jsrt, batch_size=186, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "228dac67-82bf-450d-ac3a-84a26229d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey train cxr14\n",
      "Min: -1.9025423526763916 \n",
      "Max: 2.052259922027588 \n",
      "Mean: 0.7110474705696106\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey train cxr14\")\n",
    "for data in grey_cxr14_train_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "994f3821-aea4-4fcd-9a3f-9668f16f9a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey test cxr14\n",
      "Min: -1.467163324356079 \n",
      "Max: 2.2914161682128906 \n",
      "Mean: 0.7554733157157898\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey test cxr14\")\n",
    "for data in grey_cxr_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b42244a8-0890-4844-9d5c-9b07a7199a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey test padchest\n",
      "Min: -1.6720582246780396 \n",
      "Max: 0.530906617641449 \n",
      "Mean: -0.5429261922836304\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey test padchest\")\n",
    "for data in grey_pad_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a7ce3dcb-95f7-4168-8914-f0c12697e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey OpenI\n",
      "Min: -1.6720582246780396 \n",
      "Max: 1.638906717300415 \n",
      "Mean: 0.3519674837589264\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey OpenI\")\n",
    "for data in grey_openi_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c293ff24-609b-4f18-b227-40aed635573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey JSRT\n",
      "Min: -1.6720582246780396 \n",
      "Max: 0.5439419150352478 \n",
      "Mean: -0.1966785192489624\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey JSRT\")\n",
    "for data in grey_jsrt_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9321b26-1a1c-47b1-a341-dcea0f25e0f3",
   "metadata": {},
   "source": [
    "## Use ImageNet Means for CXR Tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a9aeeec-0caa-4aa9-8f6d-8dfada548909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "crop_size = 224\n",
    "# mean = (128.2716, 128.2716, 128.2716)\n",
    "# std = (76.7148, 76.7148, 76.7148)\n",
    "mean=(0.449*255,0.449*255,0.449*255) #grey\n",
    "std=(0.236*255,0.236*255,0.236*255) #grey\n",
    "cxr_transform_list = [\n",
    "    v2.ToImage(),\n",
    "    v2.RandomRotation(15),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomApply([v2.ColorJitter(0.4, 0.2, 0.2,0)], p=0.8),\n",
    "    v2.Grayscale(num_output_channels=3),\n",
    "    v2.RandomResizedCrop(size=crop_size, scale=(0.6, 1.),antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=False),\n",
    "    v2.Normalize(mean=mean, std=std)   \n",
    "]\n",
    "grey_comp_transforms = v2.Compose(cxr_transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64d8add6-42e6-4f3a-84f7-8439cc248a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_train_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14traindir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_test_cxr14 = torchvision.datasets.ImageFolder(\n",
    "    cxr14testdir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_train_pad = torchvision.datasets.ImageFolder(\n",
    "    padtraindir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_test_pad = torchvision.datasets.ImageFolder(\n",
    "    padtestdir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_openi = torchvision.datasets.ImageFolder(\n",
    "    openidir, transform=grey_comp_transforms\n",
    ")\n",
    "grey_jsrt = torchvision.datasets.ImageFolder(\n",
    "    jsrtdir, transform=grey_comp_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c4ce833-d6d0-4bf7-8697-92b0cf6ba443",
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_cxr14_train_loader = torch.utils.data.DataLoader(grey_train_cxr14, batch_size=1638, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_cxr14_test_loader = torch.utils.data.DataLoader(grey_test_cxr14, batch_size=360, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_pad_loader = torch.utils.data.DataLoader(grey_test_pad, batch_size=188, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_openi_loader = torch.utils.data.DataLoader(grey_openi, batch_size=108, \n",
    "                          shuffle=False)\n",
    "\n",
    "grey_jsrt_loader = torch.utils.data.DataLoader(grey_jsrt, batch_size=186, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67dfa8b-a555-4dbe-b916-53d364ee296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey train cxr14\n",
      "Min: -1.9025423526763916 \n",
      "Max: 2.318129062652588 \n",
      "Mean: 1.1368833780288696\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey train cxr14\")\n",
    "for data in grey_cxr14_train_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e100a3-daa3-40f2-ace3-2163b9ea7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey test cxr14\n",
      "Min: -1.9025423526763916 \n",
      "Max: 2.0024094581604004 \n",
      "Mean: 0.29045477509498596\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey test cxr14\")\n",
    "for data in grey_cxr14_test_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f26aa00-6691-4f52-a27c-bd39bcc64aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey test padchest\n",
      "Min: -1.9025423526763916 \n",
      "Max: 1.2878862619400024 \n",
      "Mean: -0.3216048777103424\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey test padchest\")\n",
    "for data in grey_pad_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3cb6a9c-3427-41ad-bc1d-06c644f5b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey OpenI\n",
      "Min: -1.9025423526763916 \n",
      "Max: 2.3015122413635254 \n",
      "Mean: 1.227964997291565\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey OpenI\")\n",
    "for data in grey_openi_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "486c1a2a-e3ce-4e4e-b9c8-310e3ac699dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grey JSRT\n",
      "Min: -1.9025423526763916 \n",
      "Max: 1.9525588750839233 \n",
      "Mean: 0.31726306676864624\n"
     ]
    }
   ],
   "source": [
    "print(\"Grey JSRT\")\n",
    "for data in grey_jsrt_loader:\n",
    "    inputs, labels = data[0],data[1]\n",
    "    break\n",
    "print(f\"Min: {inputs[0].min()} \\\n",
    "\\nMax: {inputs[0].max()} \\\n",
    "\\nMean: {inputs[0].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38869b1-e50d-410e-9481-1a12a78722b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
